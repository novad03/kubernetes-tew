apiVersion: v1
kind: Secret
metadata:
  namespace: monitoring
  name: alertmanager-kube-prometheus
  labels:
    alertmanager: kube-prometheus
    app: alertmanager
data:
  alertmanager.yaml: "Z2xvYmFsOgogIHJlc29sdmVfdGltZW91dDogNW0KcmVjZWl2ZXJzOgotIG5hbWU6ICJudWxsIgpyb3V0ZToKICBncm91cF9ieToKICAtIGpvYgogIGdyb3VwX2ludGVydmFsOiA1bQogIGdyb3VwX3dhaXQ6IDMwcwogIHJlY2VpdmVyOiAibnVsbCIKICByZXBlYXRfaW50ZXJ2YWw6IDEyaAogIHJvdXRlczoKICAtIG1hdGNoOgogICAgICBhbGVydG5hbWU6IERlYWRNYW5zU3dpdGNoCiAgICByZWNlaXZlcjogIm51bGwiCg=="
---
apiVersion: v1
kind: Secret
metadata:
  namespace: monitoring
  name: kube-prometheus-grafana
  labels:
    app: kube-prometheus-grafana
type: Opaque
data:
  user: "YWRtaW4="
  password: "Y2hhbmdlbWU="
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-node
  labels:
    app: exporter-node
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: monitoring
  name: kube-prometheus-grafana
  labels:
    app: kube-prometheus-grafana
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: monitoring
  name: kube-prometheus
  labels:
    app: prometheus
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: psp-kube-prometheus-alertmanager
  labels:
    app: alertmanager
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - kube-prometheus-alertmanager
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - nodes
  - pods
  - services
  - resourcequotas
  - replicationcontrollers
  - limitranges
  - persistentvolumeclaims
  - persistentvolumes
  - endpoints
  verbs: ["list", "watch"]
- apiGroups: ["extensions"]
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs: ["list", "watch"]
- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]
- apiGroups: ["batch"]
  resources:
  - cronjobs
  - jobs
  verbs: ["list", "watch"]
- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: psp-kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - kube-prometheus-exporter-kube-state
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: psp-kube-prometheus-exporter-node
  labels:
    app: exporter-node
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - kube-prometheus-exporter-node
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: psp-kube-prometheus-grafana
  labels:
    app: kube-prometheus-grafana
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - kube-prometheus-grafana
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: kube-prometheus
  labels:
    app: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- apiGroups: [""]
  resources:
  - nodes/metrics
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: psp-kube-prometheus
  labels:
    app: prometheus
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - kube-prometheus
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: psp-kube-prometheus-alertmanager
  labels:
    app: alertmanager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-kube-prometheus-alertmanager
subjects:
  - kind: ServiceAccount
    name: default
    namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-prometheus-exporter-kube-state
subjects:
  - kind: ServiceAccount
    name: kube-prometheus-exporter-kube-state
    namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: psp-kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-kube-prometheus-exporter-kube-state
subjects:
  - kind: ServiceAccount
    name: kube-prometheus-exporter-kube-state
    namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: psp-kube-prometheus-exporter-node
  labels:
    app: exporter-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-kube-prometheus-exporter-node
subjects:
  - kind: ServiceAccount
    name: kube-prometheus-exporter-node
    namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: psp-kube-prometheus-grafana
  labels:
    app: kube-prometheus-grafana
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-kube-prometheus-grafana
subjects:
  - kind: ServiceAccount
    name: kube-prometheus-grafana
    namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kube-prometheus
  labels:
    app: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-prometheus
subjects:
  - kind: ServiceAccount
    name: kube-prometheus
    namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app: prometheus
  name: psp-kube-prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-kube-prometheus
subjects:
  - kind: ServiceAccount
    name: kube-prometheus
    namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get"]
- apiGroups: ["extensions"]
  resources: ["deployments"]
  resourceNames: [kube-prometheus-exporter-kube-state]
  verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kube-prometheus-exporter-kube-state
subjects:
  - kind: ServiceAccount
    name: kube-prometheus-exporter-kube-state
    namespace: monitoring
---
apiVersion: v1
kind: Service
metadata:
  namespace: monitoring
  name: kube-prometheus-alertmanager
  labels:
    alertmanager: kube-prometheus
    app: alertmanager
spec:
  ports:
    - name: http
      port: 9093
      targetPort: 9093
      protocol: TCP
  selector:
    alertmanager: kube-prometheus
    app: alertmanager
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-prometheus-exporter-kube-controller-manager
  labels:
    app: exporter-kube-controller-manager
    component: kube-controller-manager
spec:
  ports:
    - name: http-metrics
      port: 10252
      protocol: TCP
      targetPort: 10252
  selector:
    k8s-app: kube-controller-manager
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-prometheus-exporter-kube-dns
  labels:
    app: exporter-kube-dns
    component: kube-dns
spec:
  ports:
    - name: http-metrics-dnsmasq
      port: 10054
      protocol: TCP
      targetPort: 10054
    - name: http-metrics-skydns
      port: 10055
      protocol: TCP
      targetPort: 10055
  selector:
    k8s-app: kube-dns
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-prometheus-exporter-kube-etcd
  labels:
    app: exporter-kube-etcd
    component: kube-etcd
spec:
  ports:
    - name: http-metrics
      port: 4001
      protocol: TCP
      targetPort: 4001
  selector:
    k8s-app: etcd-server
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-prometheus-exporter-kube-scheduler
  labels:
    app: exporter-kube-scheduler
    component: kube-scheduler
spec:
  ports:
    - name: http-metrics
      port: 10251
      protocol: TCP
      targetPort: 10251
  selector:
    k8s-app: kube-scheduler
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
    component: kube-state
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: kube-state-metrics
  selector:
    app: kube-prometheus-exporter-kube-state
    component: kube-state
---
apiVersion: v1
kind: Service
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-node
  labels:
    app: exporter-node
    component: node-exporter
spec:
  type: ClusterIP
  ports:
  - name: metrics
    port: 9100
    targetPort: metrics
    protocol: TCP
  selector:
    app: kube-prometheus-exporter-node
    component: node-exporter
---
apiVersion: v1
kind: Service
metadata:
  namespace: monitoring
  name: kube-prometheus-grafana
  labels:
    app: kube-prometheus-grafana
spec:
  ports:
    - name: "http"
      port: 80
      protocol: TCP
      targetPort: 3000
      nodePort: [[.GrafanaPort]]
  selector:
    app: kube-prometheus-grafana
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  namespace: monitoring
  name: kube-prometheus
  labels:
    app: prometheus
    prometheus: "kube-prometheus"
spec:
  sessionAffinity: "None"
  ports:
    - name: http
      port: 9090
      targetPort: 9090
      protocol: TCP
  selector:
    app: prometheus
    prometheus: kube-prometheus
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-node
  labels:
    app: exporter-node
    component: node-exporter
spec:
  template:
    metadata:
      labels:
        app: kube-prometheus-exporter-node
        component: node-exporter
    spec:
      containers:
        - name: node-exporter
          image: "[[.PrometheusNodeExporterImage]]"
          args:
          - --web.listen-address=0.0.0.0:9100
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          ports:
            - name: metrics
              containerPort: 9100
              hostPort: 9100
          resources:
            limits:
              cpu: 200m
              memory: 50Mi
            requests:
              cpu: 100m
              memory: 30Mi
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
      serviceAccountName: kube-prometheus-exporter-node
      tolerations:
        - effect: NoSchedule
          operator: Exists
      hostNetwork: true
      hostPID: true
      volumes:
      - hostPath:
          path: /proc
        name: proc
      - hostPath:
          path: /sys
        name: sys
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
    component: kube-state
spec:
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: kube-prometheus-exporter-kube-state
  template:
    metadata:
      labels:
        app: kube-prometheus-exporter-kube-state
        component: kube-state
    spec:
      containers:
      - name: exporter-kube-state
        image: "[[.KubeStateMetricsImage]]"
        ports:
        - containerPort: 8080
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 30
          timeoutSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 30
          timeoutSeconds: 5
      - name: exporter-kube-state-addon-resizer
        image: "[[.AddonResizerImage]]"
        env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        command:
          - /pod_nanny
          - --container=exporter-kube-state
          - --cpu=100m
          - --extra-cpu=1m
          - --memory=130Mi
          - --extra-memory=2Mi
          - --threshold=5
          - --deployment=kube-prometheus-exporter-kube-state
        resources:
            limits:
              cpu: 100m
              memory: 30Mi
            requests:
              cpu: 100m
              memory: 30Mi
      serviceAccountName: kube-prometheus-exporter-kube-state
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  namespace: monitoring
  name: kube-prometheus-grafana
  labels:
    app: kube-prometheus-grafana
spec:
  replicas: 1
  template:
    metadata:
      annotations:
      labels:
        app: kube-prometheus-grafana
    spec:
      containers:
      - name: grafana
        image: "[[.GrafanaImage]]"
        env:
        - name: GF_AUTH_BASIC_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "true"
        - name: GF_SECURITY_ADMIN_USER
          valueFrom:
            secretKeyRef:
              name: kube-prometheus-grafana
              key: user
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: kube-prometheus-grafana
              key: password
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        ports:
        - name: web
          containerPort: 3000
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          periodSeconds: 1
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 10
      - name: grafana-watcher
        image: "[[.GrafanaWatcherImage]]"
        args:
          - '--watch-dir=/var/grafana-dashboards'
          - '--grafana-url=http://127.0.0.1:3000'
        env:
        - name: GRAFANA_USER
          valueFrom:
            secretKeyRef:
              name: kube-prometheus-grafana
              key: user
        - name: GRAFANA_PASSWORD
          valueFrom:
            secretKeyRef:
              name: kube-prometheus-grafana
              key: password
        volumeMounts:
          - name: kube-prometheus-datasource
            mountPath: /var/grafana-dashboards/prometheus-datasource.json
            subPath: prometheus-datasource.json
          - name: kube-prometheus-kubernetes-cluster-status-dashboard
            mountPath: /var/grafana-dashboards/kubernetes-cluster-status-dashboard.json
            subPath: kubernetes-cluster-status-dashboard.json
          - name: kube-prometheus-kubernetes-cluster-health-dashboard
            mountPath: /var/grafana-dashboards/kubernetes-cluster-health-dashboard.json
            subPath: kubernetes-cluster-health-dashboard.json
          - name: kube-prometheus-kubernetes-capacity-planning-dashboard
            mountPath: /var/grafana-dashboards/kubernetes-capacity-planning-dashboard.json
            subPath: kubernetes-capacity-planning-dashboard.json
          - name: kube-prometheus-kubernetes-resource-requests-dashboard
            mountPath: /var/grafana-dashboards/kubernetes-resource-requests-dashboard.json
            subPath: kubernetes-resource-requests-dashboard.json
          - name: kube-prometheus-kubernetes-control-plane-status-dashboard
            mountPath: /var/grafana-dashboards/kubernetes-control-plane-status-dashboard.json
            subPath: kubernetes-control-plane-status-dashboard.json
          - name: kube-prometheus-nodes-dashboard
            mountPath: /var/grafana-dashboards/nodes-dashboard.json
            subPath: nodes-dashboard.json
          - name: kube-prometheus-deployment-dashboard
            mountPath: /var/grafana-dashboards/deployment-dashboard.json
            subPath: deployment-dashboard.json
          - name: kube-prometheus-statefulset-dashboard
            mountPath: /var/grafana-dashboards/statefulset-dashboard.json
            subPath: statefulset-dashboard.json
          - name: kube-prometheus-pods-dashboard
            mountPath: /var/grafana-dashboards/pods-dashboard.json
            subPath: pods-dashboard.json
      serviceAccountName: kube-prometheus-grafana
      volumes:
        - name: grafana-storage
          emptyDir: {}
        - name: kube-prometheus-datasource
          configMap:
            name: kube-prometheus-datasource 
        - name: kube-prometheus-kubernetes-cluster-status-dashboard
          configMap:
            name: kube-prometheus-kubernetes-cluster-status-dashboard 
        - name: kube-prometheus-pods-dashboard
          configMap:
            name: kube-prometheus-pods-dashboard 
        - name: kube-prometheus-deployment-dashboard
          configMap:
            name: kube-prometheus-deployment-dashboard 
        - name: kube-prometheus-kubernetes-control-plane-status-dashboard
          configMap:
            name: kube-prometheus-kubernetes-control-plane-status-dashboard 
        - name: kube-prometheus-statefulset-dashboard
          configMap:
            name: kube-prometheus-statefulset-dashboard 
        - name: kube-prometheus-kubernetes-capacity-planning-dashboard
          configMap:
            name: kube-prometheus-kubernetes-capacity-planning-dashboard 
        - name: kube-prometheus-kubernetes-resource-requests-dashboard
          configMap:
            name: kube-prometheus-kubernetes-resource-requests-dashboard 
        - name: kube-prometheus-kubernetes-cluster-health-dashboard
          configMap:
            name: kube-prometheus-kubernetes-cluster-health-dashboard 
        - name: kube-prometheus-nodes-dashboard
          configMap:
            name: kube-prometheus-nodes-dashboard 
---
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  namespace: monitoring
  name: kube-prometheus
  labels:
    alertmanager: kube-prometheus
    app: alertmanager
spec:
  baseImage: "[[.PrometheusAlertManagerImage | image_name]]"
  externalUrl: http://kube-prometheus-alertmanager.monitoring:9093
  paused: false
  replicas: 1
  version: "[[.PrometheusAlertManagerImage | image_tag]]"
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              app: alertmanager
              alertmanager: kube-prometheus
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  namespace: monitoring
  name: kube-prometheus-alertmanager
  labels:
    app: alertmanager
spec:
  privileged: false
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
spec:
  privileged: false
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-node
  labels:
    app: exporter-node
spec:
  privileged: false
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
    - 'hostPath'
  hostNetwork: true
  hostIPC: false
  hostPID: true
  hostPorts:
    - min: 0
      max: 65535
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  namespace: monitoring
  name: kube-prometheus-grafana
  labels:
    app: kube-prometheus-grafana
spec:
  privileged: false
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
    - 'hostPath'
  hostNetwork: true
  hostIPC: false
  hostPID: true
  hostPorts:
    - min: 0
      max: 65535
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  namespace: monitoring
  name: kube-prometheus
  labels:
    app: prometheus
spec:
  privileged: false
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  namespace: monitoring
  name: kube-prometheus
  labels:
    app: prometheus
    prometheus: kube-prometheus
spec:
  alerting:
    alertmanagers:
      - namespace: monitoring
        name: kube-prometheus-alertmanager
        port: http
  baseImage: "[[.PrometheusImage | image_name]]"
  externalUrl: http://kube-prometheus.monitoring:9090
  paused: false
  replicas: 1
  logLevel:  info
  retention: "24h"
  routePrefix: "/"
  serviceAccountName: kube-prometheus
  serviceMonitorSelector:
    matchLabels:
      prometheus: "kube-prometheus"
  ruleSelector:
    matchLabels:
      prometheus: "kube-prometheus"
  version: "[[.PrometheusImage | image_tag]]"
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              app: prometheus
              prometheus: "kube-prometheus"
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-alertmanager
  labels:
    app: "alertmanager"
    prometheus: "kube-prometheus"
spec:
  groups:
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerConfigInconsistent
      expr: count_values("config_hash", alertmanager_config_hash) BY (service) / ON(service)
        GROUP_LEFT() label_replace(prometheus_operator_alertmanager_spec_replicas, "service",
        "alertmanager-$1", "alertmanager", "(.*)") != 1
      for: 5m
      labels:
        severity: critical
      annotations:
        description: The configuration of the instances of the Alertmanager cluster
          `{{$labels.service}}` are out of sync.
        summary: Configuration out of sync
    - alert: AlertmanagerDownOrMissing
      expr: label_replace(prometheus_operator_alertmanager_spec_replicas, "job", "alertmanager-$1",
        "alertmanager", "(.*)") / ON(job) GROUP_RIGHT() sum(up) BY (job) != 1
      for: 5m
      labels:
        severity: warning
      annotations:
        description: An unexpected number of Alertmanagers are scraped or Alertmanagers
          disappeared from discovery.
        summary: Alertmanager down or missing
    - alert: AlertmanagerFailedReload
      expr: alertmanager_config_last_reload_successful == 0
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
          }}/{{ $labels.pod}}.
        summary: Alertmanager's configuration reload failed
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-controller-manager
  labels:
    app: "prometheus"
    prometheus: kube-prometheus
spec:
  groups:
  - name: kube-controller-manager.rules
    rules:
    - alert: K8SControllerManagerDown
      expr: absent(up{job="kube-controller-manager"} == 1)
      for: 5m
      labels:
        severity: critical
      annotations:
        description: There is no running K8S controller manager. Deployments and replication
          controllers are not making progress.
        runbook: https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-controller-manager
        summary: Controller manager is down
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-etcd
  labels:
    app: "prometheus"
    prometheus: kube-prometheus
spec:
  groups:
  - name: ./etcd3.rules
    rules:
    - alert: InsufficientMembers
      expr: count(up{job="kube-etcd"} == 0) > (count(up{job="kube-etcd"}) / 2 - 1)
      for: 3m
      labels:
        severity: critical
      annotations:
        description: If one more etcd member goes down the cluster will be unavailable
        summary: etcd cluster insufficient members
    - alert: NoLeader
      expr: etcd_server_has_leader{job="kube-etcd"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        description: etcd member {{ $labels.instance }} has no leader
        summary: etcd member has no leader
    - alert: HighNumberOfLeaderChanges
      expr: increase(etcd_server_leader_changes_seen_total{job="kube-etcd"}[1h]) > 3
      labels:
        severity: warning
      annotations:
        description: etcd instance {{ $labels.instance }} has seen {{ $value }} leader
          changes within the last hour
        summary: a high number of leader changes within the etcd cluster are happening
    - alert: HighNumberOfFailedGRPCRequests
      expr: sum(rate(grpc_server_handled_total{grpc_code!="OK",job="kube-etcd"}[5m])) BY (grpc_service, grpc_method)
        / sum(rate(grpc_server_handled_total{job="kube-etcd"}[5m])) BY (grpc_service, grpc_method) > 0.01
      for: 10m
      labels:
        severity: warning
      annotations:
        description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed
          on etcd instance {{ $labels.instance }}'
        summary: a high number of gRPC requests are failing
    - alert: HighNumberOfFailedGRPCRequests
      expr: sum(rate(grpc_server_handled_total{grpc_code!="OK",job="kube-etcd"}[5m])) BY (grpc_service, grpc_method)
        / sum(rate(grpc_server_handled_total{job="kube-etcd"}[5m])) BY (grpc_service, grpc_method) > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed
          on etcd instance {{ $labels.instance }}'
        summary: a high number of gRPC requests are failing
    - alert: GRPCRequestsSlow
      expr: histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job="kube-etcd",grpc_type="unary"}[5m])) by (grpc_service, grpc_method, le))
        > 0.15
      for: 10m
      labels:
        severity: critical
      annotations:
        description: on etcd instance {{ $labels.instance }} gRPC requests to {{ $labels.grpc_method
          }} are slow
        summary: slow gRPC requests
    - alert: HighNumberOfFailedHTTPRequests
      expr: sum(rate(etcd_http_failed_total{job="kube-etcd"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job="kube-etcd"}[5m]))
        BY (method) > 0.01
      for: 10m
      labels:
        severity: warning
      annotations:
        description: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd
          instance {{ $labels.instance }}'
        summary: a high number of HTTP requests are failing
    - alert: HighNumberOfFailedHTTPRequests
      expr: sum(rate(etcd_http_failed_total{job="kube-etcd"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job="kube-etcd"}[5m]))
        BY (method) > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        description: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd
          instance {{ $labels.instance }}'
        summary: a high number of HTTP requests are failing
    - alert: HTTPRequestsSlow
      expr: histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m]))
        > 0.15
      for: 10m
      labels:
        severity: warning
      annotations:
        description: on etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method
          }} are slow
        summary: slow HTTP requests
    - alert: EtcdMemberCommunicationSlow
      expr: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))
        > 0.15
      for: 10m
      labels:
        severity: warning
      annotations:
        description: etcd instance {{ $labels.instance }} member communication with
          {{ $labels.To }} is slow
        summary: etcd member communication is slow
    - alert: HighNumberOfFailedProposals
      expr: increase(etcd_server_proposals_failed_total{job="kube-etcd"}[1h]) > 5
      labels:
        severity: warning
      annotations:
        description: etcd instance {{ $labels.instance }} has seen {{ $value }} proposal
          failures within the last hour
        summary: a high number of proposals within the etcd cluster are failing
    - alert: HighFsyncDurations
      expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m]))
        > 0.5
      for: 10m
      labels:
        severity: warning
      annotations:
        description: etcd instance {{ $labels.instance }} fync durations are high
        summary: high fsync durations
    - alert: HighCommitDurations
      expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m]))
        > 0.25
      for: 10m
      labels:
        severity: warning
      annotations:
        description: etcd instance {{ $labels.instance }} commit durations are high
        summary: high commit durations
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-scheduler
  labels:
    app: "prometheus"
    prometheus: kube-prometheus
spec:
  groups:
  - name: kube-scheduler.rules
    rules:
    - record: cluster:scheduler_e2e_scheduling_latency_seconds:quantile
      expr: histogram_quantile(0.99, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.99"
    - record: cluster:scheduler_e2e_scheduling_latency_seconds:quantile
      expr: histogram_quantile(0.9, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.9"
    - record: cluster:scheduler_e2e_scheduling_latency_seconds:quantile
      expr: histogram_quantile(0.5, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.5"
    - record: cluster:scheduler_scheduling_algorithm_latency_seconds:quantile
      expr: histogram_quantile(0.99, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.99"
    - record: cluster:scheduler_scheduling_algorithm_latency_seconds:quantile
      expr: histogram_quantile(0.9, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.9"
    - record: cluster:scheduler_scheduling_algorithm_latency_seconds:quantile
      expr: histogram_quantile(0.5, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.5"
    - record: cluster:scheduler_binding_latency_seconds:quantile
      expr: histogram_quantile(0.99, sum(scheduler_binding_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.99"
    - record: cluster:scheduler_binding_latency_seconds:quantile
      expr: histogram_quantile(0.9, sum(scheduler_binding_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.9"
    - record: cluster:scheduler_binding_latency_seconds:quantile
      expr: histogram_quantile(0.5, sum(scheduler_binding_latency_microseconds_bucket)
        BY (le, cluster)) / 1e+06
      labels:
        quantile: "0.5"
    - alert: K8SSchedulerDown
      expr: absent(up{job="kube-scheduler"} == 1)
      for: 5m
      labels:
        severity: critical
      annotations:
        description: There is no running K8S scheduler. New pods are not being assigned
          to nodes.
        runbook: https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-scheduler
        summary: Scheduler is down
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-state
  labels:
    app: "prometheus"
    prometheus: kube-prometheus
spec:
  groups:
  - name: kube-state-metrics.rules
    rules:
    - alert: DeploymentGenerationMismatch
      expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
      for: 15m
      labels:
        severity: warning
      annotations:
        description: Observed deployment generation does not match expected one for
          deployment {{$labels.namespace}}/{{$labels.deployment}}
        summary: Deployment is outdated
    - alert: DeploymentReplicasNotUpdated
      expr: ((kube_deployment_status_replicas_updated != kube_deployment_spec_replicas)
        or (kube_deployment_status_replicas_available != kube_deployment_spec_replicas))
        unless (kube_deployment_spec_paused == 1)
      for: 15m
      labels:
        severity: warning
      annotations:
        description: Replicas are not updated and available for deployment {{$labels.namespace}}/{{$labels.deployment}}
        summary: Deployment replicas are outdated
    - alert: DaemonSetRolloutStuck
      expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled
        * 100 < 100
      for: 15m
      labels:
        severity: warning
      annotations:
        description: Only {{$value}}% of desired pods scheduled and ready for daemon
          set {{$labels.namespace}}/{{$labels.daemonset}}
        summary: DaemonSet is missing pods
    - alert: K8SDaemonSetsNotScheduled
      expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled
        > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        description: A number of daemonsets are not scheduled.
        summary: Daemonsets are not scheduled correctly
    - alert: DaemonSetsMissScheduled
      expr: kube_daemonset_status_number_misscheduled > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        description: A number of daemonsets are running where they are not supposed
          to run.
        summary: Daemonsets are not scheduled correctly
    - alert: PodFrequentlyRestarting
      expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value}}
          times within the last hour
        summary: Pod is restarting frequently
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kubelets
  labels:
    app: "prometheus"
    prometheus: kube-prometheus
spec:
  groups:
  - name: kubelet.rules
    rules:
    - alert: K8SNodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 1h
      labels:
        severity: warning
      annotations:
        description: The Kubelet on {{ $labels.node }} has not checked in with the API,
          or has set itself to NotReady, for more than an hour
        summary: Node status is NotReady
    - alert: K8SManyNodesNotReady
      expr: count(kube_node_status_condition{condition="Ready",status="true"} == 0)
        > 1 and (count(kube_node_status_condition{condition="Ready",status="true"} ==
        0) / count(kube_node_status_condition{condition="Ready",status="true"})) * 100 > 20
      for: 1m
      labels:
        severity: critical
      annotations:
        description: '{{ $value }}% of Kubernetes nodes are not ready'
    - alert: K8SKubeletDown
      expr: count(up{job="kubelet"} == 0) / count(up{job="kubelet"}) * 100 > 3
      for: 1h
      labels:
        severity: warning
      annotations:
        description: Prometheus failed to scrape {{ $value }}% of kubelets.
        summary: Prometheus failed to scrape
    - alert: K8SKubeletDown
      expr: (absent(up{job="kubelet"} == 1) or count(up{job="kubelet"} == 0) / count(up{job="kubelet"}))
        * 100 > 10
      for: 1h
      labels:
        severity: critical
      annotations:
        description: Prometheus failed to scrape {{ $value }}% of kubelets, or all Kubelets
          have disappeared from service discovery.
        summary: Many Kubelets cannot be scraped
    - alert: K8SKubeletTooManyPods
      expr: kubelet_running_pod_count > 100
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Kubelet {{$labels.instance}} is running {{$value}} pods, close
          to the limit of 110
        summary: Kubelet is close to pod limit
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kubernetes
  labels:
    app: "prometheus"
    prometheus: kube-prometheus
spec:
  groups:
  - name: kubernetes.rules
    rules:
    - record: pod_name:container_memory_usage_bytes:sum
      expr: sum(container_memory_usage_bytes{container_name!="POD",pod_name!=""}) BY
        (pod_name)
    - record: pod_name:container_spec_cpu_shares:sum
      expr: sum(container_spec_cpu_shares{container_name!="POD",pod_name!=""}) BY (pod_name)
    - record: pod_name:container_cpu_usage:sum
      expr: sum(rate(container_cpu_usage_seconds_total{container_name!="POD",pod_name!=""}[5m]))
        BY (pod_name)
    - record: pod_name:container_fs_usage_bytes:sum
      expr: sum(container_fs_usage_bytes{container_name!="POD",pod_name!=""}) BY (pod_name)
    - record: namespace:container_memory_usage_bytes:sum
      expr: sum(container_memory_usage_bytes{container_name!=""}) BY (namespace)
    - record: namespace:container_spec_cpu_shares:sum
      expr: sum(container_spec_cpu_shares{container_name!=""}) BY (namespace)
    - record: namespace:container_cpu_usage:sum
      expr: sum(rate(container_cpu_usage_seconds_total{container_name!="POD"}[5m]))
        BY (namespace)
    - record: cluster:memory_usage:ratio
      expr: sum(container_memory_usage_bytes{container_name!="POD",pod_name!=""}) BY
        (cluster) / sum(machine_memory_bytes) BY (cluster)
    - record: cluster:container_spec_cpu_shares:ratio
      expr: sum(container_spec_cpu_shares{container_name!="POD",pod_name!=""}) / 1000
        / sum(machine_cpu_cores)
    - record: cluster:container_cpu_usage:ratio
      expr: sum(rate(container_cpu_usage_seconds_total{container_name!="POD",pod_name!=""}[5m]))
        / sum(machine_cpu_cores)
    - record: apiserver_latency_seconds:quantile
      expr: histogram_quantile(0.99, rate(apiserver_request_latencies_bucket[5m])) /
        1e+06
      labels:
        quantile: "0.99"
    - record: apiserver_latency:quantile_seconds
      expr: histogram_quantile(0.9, rate(apiserver_request_latencies_bucket[5m])) /
        1e+06
      labels:
        quantile: "0.9"
    - record: apiserver_latency_seconds:quantile
      expr: histogram_quantile(0.5, rate(apiserver_request_latencies_bucket[5m])) /
        1e+06
      labels:
        quantile: "0.5"
    - alert: APIServerLatencyHigh
      expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"}
        > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        description: the API server has a 99th percentile latency of {{ $value }} seconds
          for {{$labels.verb}} {{$labels.resource}}
        summary: API server high latency
    - alert: APIServerLatencyHigh
      expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"}
        > 4
      for: 10m
      labels:
        severity: critical
      annotations:
        description: the API server has a 99th percentile latency of {{ $value }} seconds
          for {{$labels.verb}} {{$labels.resource}}
        summary: API server high latency
    - alert: APIServerErrorsHigh
      expr: rate(apiserver_request_count{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_count[5m])
        * 100 > 2
      for: 10m
      labels:
        severity: warning
      annotations:
        description: API server returns errors for {{ $value }}% of requests
        summary: API server request errors
    - alert: APIServerErrorsHigh
      expr: rate(apiserver_request_count{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_count[5m])
        * 100 > 5
      for: 10m
      labels:
        severity: critical
      annotations:
        description: API server returns errors for {{ $value }}% of requests
    - alert: K8SApiserverDown
      expr: absent(up{job="apiserver"} == 1)
      for: 20m
      labels:
        severity: critical
      annotations:
        description: No API servers are reachable or all have disappeared from service
          discovery
        summary: No API servers are reachable
    - alert: K8sCertificateExpirationNotice
      labels:
        severity: warning
      annotations:
        description: Kubernetes API Certificate is expiring soon (less than 7 days)
        summary: Kubernetes API Certificate is expiering soon
      expr: sum(apiserver_client_certificate_expiration_seconds_bucket{le="604800"}) > 0
    - alert: K8sCertificateExpirationNotice
      labels:
        severity: critical
      annotations:
        description: Kubernetes API Certificate is expiring in less than 1 day
        summary: Kubernetes API Certificate is expiering
      expr: sum(apiserver_client_certificate_expiration_seconds_bucket{le="86400"}) > 0
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-node
  labels:
    app: "prometheus"
    prometheus: kube-prometheus
spec:
  groups:
  - name: node.rules
    rules:
    - record: instance:node_cpu:rate:sum
      expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[3m]))
        BY (instance)
    - record: instance:node_filesystem_usage:sum
      expr: sum((node_filesystem_size{mountpoint="/"} - node_filesystem_free{mountpoint="/"}))
        BY (instance)
    - record: instance:node_network_receive_bytes:rate:sum
      expr: sum(rate(node_network_receive_bytes[3m])) BY (instance)
    - record: instance:node_network_transmit_bytes:rate:sum
      expr: sum(rate(node_network_transmit_bytes[3m])) BY (instance)
    - record: instance:node_cpu:ratio
      expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m])) WITHOUT (cpu, mode) / ON(instance)
        GROUP_LEFT() count(sum(node_cpu) BY (instance, cpu)) BY (instance)
    - record: cluster:node_cpu:sum_rate5m
      expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m]))
    - record: cluster:node_cpu:ratio
      expr: cluster:node_cpu:rate5m / count(sum(node_cpu) BY (instance, cpu))
    - alert: NodeExporterDown
      expr: absent(up{job="node-exporter"} == 1)
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Prometheus could not scrape a node-exporter for more than 10m,
          or node-exporters have disappeared from discovery
        summary: Prometheus could not scrape a node-exporter
    - alert: NodeDiskRunningFull
      expr: predict_linear(node_filesystem_free{job="node-exporter",mountpoint!~"^/etc/(?:resolv.conf|hosts|hostname)$"}[6h], 3600 * 24) < 0 and on(instance) up{job="node-exporter"}
      for: 30m
      labels:
        severity: warning
      annotations:
        description: device {{$labels.device}} on node {{$labels.instance}} is running
          full within the next 24 hours (mounted at {{$labels.mountpoint}})
        summary: Node disk is running full within 24 hours
    - alert: NodeDiskRunningFull
      expr: predict_linear(node_filesystem_free{job="node-exporter",mountpoint!~"^/etc/(?:resolv.conf|hosts|hostname)$"}[30m], 3600 * 2) < 0 and on(instance) up{job="node-exporter"}
      for: 10m
      labels:
        severity: critical
      annotations:
        description: device {{$labels.device}} on node {{$labels.instance}} is running
          full within the next 2 hours (mounted at {{$labels.mountpoint}})
        summary: Node disk is running full within 2 hours
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus-rules
  labels:
    app: "prometheus"
    prometheus: "kube-prometheus"
spec:
  groups:
  - name: prometheus.rules
    rules:
    - alert: PrometheusConfigReloadFailed
      expr: prometheus_config_last_reload_successful == 0
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
        summary: Reloading Promehteus' configuration failed
    - alert: PrometheusNotificationQueueRunningFull
      expr: predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{
          $labels.pod}}
        summary: Prometheus' alert notification queue is running full  
    - alert: PrometheusErrorSendingAlerts
      expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
        > 0.01
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
          $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
        summary: Errors while sending alert from Prometheus
    - alert: PrometheusErrorSendingAlerts
      expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
        > 0.03
      for: 10m
      labels:
        severity: critical
      annotations:
        description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
          $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
        summary: Errors while sending alerts from Prometheus
    - alert: PrometheusNotConnectedToAlertmanagers
      expr: prometheus_notifications_alertmanagers_discovered < 1
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected
          to any Alertmanagers
        summary: Prometheus is not connected to any Alertmanagers
    - alert: PrometheusTSDBReloadsFailing
      expr: increase(prometheus_tsdb_reloads_failures_total[2h]) > 0
      for: 12h
      labels:
        severity: warning
      annotations:
        description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
          reload failures over the last four hours.'
        summary: Prometheus has issues reloading data blocks from disk
    - alert: PrometheusTSDBCompactionsFailing
      expr: increase(prometheus_tsdb_compactions_failed_total[2h]) > 0
      for: 12h
      labels:
        severity: warning
      annotations:
        description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
          compaction failures over the last four hours.'
        summary: Prometheus has issues compacting sample blocks
    - alert: PrometheusTSDBWALCorruptions
      expr: tsdb_wal_corruptions_total > 0
      for: 4h
      labels:
        severity: warning
      annotations:
        description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead
          log (WAL).'
        summary: Prometheus write-ahead log is corrupted
    - alert: PrometheusNotIngestingSamples
      expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
      for: 10m
      labels:
        severity: warning
      annotations:
        description: "Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting samples."
        summary: "Prometheus isn't ingesting samples"
    - alert: PrometheusTargetScrapesDuplicate
      expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        description: "{{$labels.namespace}}/{{$labels.pod}} has many samples rejected due to duplicate timestamps but different values"
        summary: Prometheus has many samples rejected
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: monitoring
  name: kube-prometheus
  labels:
    app: "prometheus"
    prometheus: kube-prometheus
spec:
  groups:
  - name: general.rules
    rules:
    - alert: TargetDown
      expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10
      for: 10m
      labels:
        severity: warning
      annotations:
        description: '{{ $value }}% of {{ $labels.job }} targets are down.'
        summary: Targets are down
    - alert: DeadMansSwitch
      expr: vector(1)
      labels:
        severity: none
      annotations:
        description: This is a DeadMansSwitch meant to ensure that the entire Alerting
          pipeline is functional.
        summary: Alerting DeadMansSwitch
    - record: fd_utilization
      expr: process_open_fds / process_max_fds
    - alert: FdExhaustionClose
      expr: predict_linear(fd_utilization[1h], 3600 * 4) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} instance
          will exhaust in file/socket descriptors within the next 4 hours'
        summary: file descriptors soon exhausted
    - alert: FdExhaustionClose
      expr: predict_linear(fd_utilization[10m], 3600) > 1
      for: 10m
      labels:
        severity: critical
      annotations:
        description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} instance
          will exhaust in file/socket descriptors within the next hour'
        summary: file descriptors soon exhausted
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-alertmanager
  labels:
    app: alertmanager
    component: alertmanager
    prometheus: "kube-prometheus"
spec:
  jobLabel: app
  selector:
    matchLabels:
      alertmanager: kube-prometheus
      app: alertmanager
  namespaceSelector:
    matchNames:
      - "monitoring"
  endpoints:
  - port: http
    interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-controller-manager
  labels:
    app: exporter-kube-controller-manager
    component: kube-controller-manager
    prometheus: kube-prometheus
spec:
  jobLabel: component
  selector:
    matchLabels:
      app: exporter-kube-controller-manager
      component: kube-controller-manager
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    interval: 15s
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-dns
  labels:    
    app: exporter-kube-dns
    component: kube-dns
    prometheus: kube-prometheus    
spec:
  jobLabel: component
  selector:
    matchLabels:
      app: exporter-kube-dns
      component: kube-dns
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics-dnsmasq
    interval: 15s
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
  - port: http-metrics-skydns
    interval: 15s
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-etcd
  labels:
    app: exporter-kube-etcd
    component: kube-etcd
    prometheus: kube-prometheus    
spec:
  jobLabel: component
  selector:
    matchLabels:
      app: exporter-kube-etcd
      component: kube-etcd
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    interval: 15s
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-scheduler
  labels:
    app: exporter-kube-scheduler
    component: kube-scheduler
    prometheus: kube-prometheus    
spec:
  jobLabel: component
  selector:
    matchLabels:
      app: exporter-kube-scheduler
      component: kube-scheduler
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: http-metrics
    interval: 15s
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kube-state
  labels:
    app: exporter-kube-state
    component: kube-state
    prometheus: kube-prometheus    
spec:
  jobLabel: component
  selector:
    matchLabels:
      app: exporter-kube-state
      component: kube-state
  namespaceSelector:
    matchNames:
      - "monitoring"
  endpoints:
  - port: kube-state-metrics
    interval: 15s
    honorLabels: true
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kubelets
  labels:
    app: exporter-kubelets
    component: kubelets
    prometheus: kube-prometheus    
spec:
  jobLabel: component
  selector:
    matchLabels:
      k8s-app: kubelet
  namespaceSelector:
    matchNames:
      - "kube-system"
  endpoints:
  - port: https-metrics
    scheme: https
    interval: 15s
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
  - port: https-metrics
    scheme: https
    path: /metrics/cadvisor
    interval: 30s
    honorLabels: true
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-kubernetes
  labels:
    app: exporter-kubernetes
    component: kubelets
    prometheus: kube-prometheus    
spec:
  jobLabel: component
  selector:
    matchLabels:
      component: apiserver
      provider: kubernetes
  namespaceSelector:
    matchNames:
      - "default"
  endpoints:
  - port: https
    interval: 15s
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-exporter-node
  labels:
    app: exporter-node
    component: node-exporter
    prometheus: kube-prometheus    
spec:
  jobLabel: component
  selector:
    matchLabels:
      app: exporter-node
      component: node-exporter
  namespaceSelector:
    matchNames:
      - "monitoring"
  endpoints:
  - port: metrics
    interval: 15s
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus-grafana
  labels:
    app: grafana
    component: grafana
    prometheus: kube-prometheus    
spec:
  jobLabel: kube-prometheus-grafana
  selector:
    matchLabels:
      app: kube-prometheus-grafana
  namespaceSelector:
    matchNames:
      - "monitoring"
  endpoints:
  - port: http
    interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: monitoring
  name: kube-prometheus
  labels:
    app: prometheus
    prometheus: "kube-prometheus"
spec:
  jobLabel: app
  selector:
    matchLabels:
      app: prometheus
      prometheus: "kube-prometheus"
  namespaceSelector:
    matchNames:
      - "monitoring"
  endpoints:
  - port: http
    interval: 30s
